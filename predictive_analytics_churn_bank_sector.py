# -*- coding: utf-8 -*-
"""revisi-predictive-analytics-churn-bank-sector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Sm94bX90Ubz1KemTZLl0cPoH2yKTeRz

>About Dataset

* RowNumber—corresponds to the record (row) number and has no effect on the output.

* CustomerId—contains random values and has no effect on customer leaving the bank.

* Surname—the surname of a customer has no impact on their decision to leave the bank.

* CreditScore—can have an effect on customer churn, since a customer with a higher credit score is less likely to leave the bank.

* Geography—a customer’s location can affect their decision to leave the bank.

* Gender—it’s interesting to explore whether gender plays a role in a customer leaving the bank.

* Age—this is certainly relevant, since older customers are less likely to leave their bank than younger ones.

* Tenure—refers to the number of years that the customer has been a client of the bank. Normally, older clients are more loyal and less likely to leave a bank.
* Balance—also a very good indicator of customer churn, as people with a higher balance in their accounts are less likely to leave the bank compared to those with lower balances.
* NumOfProducts—refers to the number of products that a customer has purchased through the bank.
* HasCrCard—denotes whether or not a customer has a credit card. This column is also relevant, since people with a credit card are less likely to leave the bank.
* IsActiveMember—active customers are less likely to leave the bank.
* EstimatedSalary—as with balance, people with lower salaries are more likely to leave the bank compared to those with higher salaries.
* Exited—whether or not the customer left the bank.

Import all the necessary libraries needed for analysis and modelling.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 
# %matplotlib inline 


from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn import svm
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import plot_confusion_matrix

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score

"""Read data using pandas library. we can see the data has 14 features.
We use .head() function to view the first 5 columns.

"""

df = pd.read_csv('churn.csv')
df.head()

"""The .info() function is used to print a concise summary of a DataFrame. This method prints information about a DataFrame. """

df.info()

""".describe().T is methode to know the statistical overview from the data"""

df.describe().T

""" The data has 10000 rows and 14 columns"""

df.shape

"""This methode is used to check the null data from each column. there are no missing value from this dataset. """

df.isnull().sum()

""".dtypes: return the dtypes in the DataFrame from each column."""

df.dtypes

df.columns

"""Since "RowNumber," "CustomerId," and "Surname" columns are not required in this analysis project because they have no bearing on the issue to be solved. We can eliminate that column using .drop method with the parameter inplace=True to  modified in place, which means it will return nothing and the dataframe is now updated.

"""

df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)

df.head()

plt.rcParams['figure.figsize'] = (20,20)
df.hist()

"""Analyse the distribution 'IsActiveMember', 'HasCrCard', 'Geography', and 'Gender' to Exited"""

fig, axarr = plt.subplots(2, 2, figsize=(14, 10))
sns.countplot(x='IsActiveMember', hue = 'Exited',data = df, ax=axarr[0][0])
sns.countplot(x='HasCrCard', hue = 'Exited',data = df, ax=axarr[0][1])
sns.countplot(x='Geography', hue = 'Exited',data = df, ax=axarr[1][0])
sns.countplot(x='Gender', hue = 'Exited',data = df, ax=axarr[1][1])

"""We can observe: 
>Clients who have been with the bank for a short period of time or a long period of time are more likely to churn than those who have been with the bank for a longer period of time.


>Female clients churn at a higher rate than male customers.

>The majority of clients that churned used credit cards.

>The vast majority of the information comes from French citizens. However, the proportion of churned customers is inversely linked to the population of consumers, implying that the bank may be experiencing a difficulty (maybe a lack of customer service personnel) in locations where it has fewer clients.

Correlation Matrix 
From the visualization below we can see that “Age” feature was found to have the strongest relationship with Exited(0.29). Also, there is a strong relationship between exited and balance variable (0.12).
"""

fig, ax = plt.subplots(figsize= [12,5])
g = sns.heatmap(df[["CreditScore","Age","Tenure","NumOfProducts","HasCrCard","Balance", "EstimatedSalary", "Exited"]].corr(),annot=True, fmt = ".2f", ax=ax, cmap = "RdPu")
ax.set_title("Correlation Matrix", fontsize=16)
plt.show()

"""Basic visualization using a boxplot of the seaborn library to discover the existence of outliers in the 'Age' column. Examine the graphic below to identify the outliers."""

plt.figure(figsize=(7,8))
Exited = df["Exited"].replace({0: "False", 1: "True"}, inplace=False)
sns.boxplot(data = df, x="Age", y=Exited)

len(df['Age'])

"""From the boxplot above we can see there is some outliers.  To remove the outliers we can used IQR (Inter Quartile Range). Inter Quartile Range approach to finding the outliers is the most commonly used and most trusted approach used in the research field.



```
iqr = q3 - q1
```

To define the outlier base value is defined above and below datasets normal range namely Upper and Lower bounds, define the upper and the lower bound (1.5*IQR value is considered) :
```
lower= q1-1.5 * iqr
upper = q3 + 1.5 * iqr
```
"""

q1 = df['Age'].quantile(0.25)
q3 = df['Age'].quantile(0.75)
iqr = q3 - q1
lower= q1-1.5 * iqr
upper = q3 + 1.5 * iqr
new_age = df.loc[(df['Age'] > lower) & (df['Age'] < upper)]
len(new_age)

"""There's not any outliers in Balance."""

plt.figure(figsize=(7,8))
sns.boxplot(data = df, x="Balance", y=Exited)

"""The vast majority of the information comes from French citizens. However, the proportion of churned customers is inversely linked to the population of consumers, implying that the bank may be experiencing a difficulty (maybe a lack of customer service personnel) in locations where it has fewer clients."""

plt.figure(figsize=(8,7))
sns.countplot(data=df, x="Geography", hue=Exited)

plt.figure(figsize=(8,7))
sns.countplot(data=df, x="IsActiveMember", hue=Exited)

plt.figure(figsize=(8,7))
sns.countplot(data=df, x="NumOfProducts", hue=Exited)

"""The visualization above shows that clients who buy more than two things have a high rate of loss, however keep in mind that our data is unreliable. All of the consumers (less than 500 people) who purchased 4 products left the bank.

#Encoding Categorical Data
"""

df['Geography'].unique()

"""pd.get_dummies(), allows you to easily one-hot encode categorical data."""

df = pd.get_dummies(df, drop_first=True)
df.head()

df['Exited'].value_counts()

from matplotlib import rcParams
rcParams['figure.figsize'] = 7,8
sns.countplot(df['Exited'])

"""From the visualizzation above there is   10000  records, of  which 7963 are  non-churners  and 2037 are churners. Therefore, the dataset is highly unbalanced in terms of the proportion of churners and non-churners.

#Handling imbalance data

---
"""

X = df.drop('Exited', axis=1)
y = df['Exited']

from imblearn.over_sampling import SMOTE 
X_smote, y_smote = SMOTE().fit_resample(X, y)
y_smote.value_counts()

"""Split data"""

X_train, X_test, y_train, y_test = train_test_split(X_smote,y_smote,test_size=0.20,random_state=42)

"""Feature Scaling """

from sklearn.preprocessing import StandardScaler
standard_scaler = StandardScaler()
X_train = standard_scaler.fit_transform(X_train)
X_test = standard_scaler.transform(X_test)

X_train

"""#Modeling and Evaluation

**Logistic Regression**
"""

model1 = LogisticRegression(C=0.03359818286283781, penalty='l1', solver='saga')
model1.fit(X_train, y_train)
prediction1 = model1.predict(X_test)
print(classification_report(y_test, prediction1))

from sklearn.metrics import accuracy_score
acc_lr = accuracy_score(y_test, prediction1)
print(acc_lr)

print(confusion_matrix(y_test, prediction1))

roc_lr  = roc_auc_score(y_test, prediction1)
print(roc_lr )

acc_lr = accuracy_score(y_test, prediction1)
recall_lr = recall_score(y_test, prediction1)
precision_lr = precision_score(y_test, prediction1)
f1_score_lr = f1_score(y_test, prediction1)

"""
**SVM**"""

from sklearn import svm
model2 = svm.SVC(gamma='auto', probability=True)
model2.fit(X_train,y_train)
prediction2 = model2.predict(X_test)
print(classification_report(y_test, prediction2))

acc_svm = accuracy_score(y_test, prediction2)
recall_svm = recall_score(y_test, prediction2)
precision_svm = precision_score(y_test, prediction2) 
f1_score_svm = f1_score(y_test, prediction2)

print(confusion_matrix(y_test, prediction2))

roc_svm  = roc_auc_score(y_test, prediction2)
print(roc_svm )

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC


param_grid = {'C': [0.1, 1, 10, 100, 1000], 
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']} 
  
grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)
grid.fit(X_train, y_train)

print(grid.best_params_)

print(grid.best_estimator_)

grid.best_estimator_

grid2 = GridSearchCV(estimator=SVC(C=10, gamma=0.1),
             param_grid={'C': [10], 'gamma': [0.1], 'kernel': ['rbf']},
             verbose=3)

grid2.fit(X_train, y_train)
grid_predictions = grid2.predict(X_test)

prediction_grid2 = grid2.predict(X_test)
acc_grid_svm = accuracy_score(y_test, prediction_grid2)
print(acc_grid_svm)

print(classification_report(y_test, grid_predictions))

"""**Decision Tree**"""

model3 =  DecisionTreeClassifier()
model3.fit(X_train,y_train)
prediction3 = model3.predict(X_test)
print(classification_report(y_test, prediction3))

model3 =  DecisionTreeClassifier()
model3.fit(X_train,y_train)
prediction3 = model3.predict(X_test)
print(classification_report(y_test, prediction3))

acc_dt = accuracy_score(y_test, prediction3)
acc_dt = accuracy_score(y_test, prediction3)
recall_dt = recall_score(y_test, prediction3)
precision_dt = precision_score(y_test, prediction3)
f1_score_dt = f1_score(y_test, prediction3)  
print(acc_dt)

print(confusion_matrix(y_test, prediction3))

roc_dt = roc_auc_score(y_test, prediction3)
print(roc_dt)

"""Decision Tree Tuning"""

from sklearn.model_selection import GridSearchCV

param_grid = {'max_features': ['auto', 'sqrt', 'log2'],
              'ccp_alpha': [0.1, .01, .001],
              'max_depth' : [5, 6, 7, 8, 9],
              'criterion' :['gini', 'entropy']
             }
dt_model = DecisionTreeClassifier(random_state=0)
search = GridSearchCV(estimator=dt_model, param_grid=param_grid, cv=5, verbose=True)
search.fit(X_train, y_train)

search.best_estimator_

dt_model = DecisionTreeClassifier(ccp_alpha=0.001, criterion='entropy', max_depth=8,
                       max_features='auto', random_state=0)
dt_model.fit(X_train, y_train)
predict_dt = dt_model.predict(X_test)

print(classification_report(y_test, predict_dt ))

"""Gradient Boosting Classifier"""

model4 = GradientBoostingClassifier()
model4.fit(X_train,y_train)
prediction4 = model4.predict(X_test)
print(classification_report(y_test, prediction4))

acc_gb = accuracy_score(y_test, prediction4)
print(acc_gb)

print(confusion_matrix(y_test, prediction4))

roc_gb = roc_auc_score(y_test, prediction4)
print(roc_gb )

"""###Using GridSearchCV to increase the accuracy in GradientBoosting"""

parameters = {'learning_rate': [0.01,0.02,0.03],
               'subsample'    : [0.9, 0.5, 0.2],
                'n_estimators' : [100,500,1000],
                'max_depth'    : [4,6,8]
              }

grid_model4 = GridSearchCV(estimator=model4, param_grid = parameters, cv = 2, n_jobs=-1)
grid_model4.fit(X_train, y_train)

grid_model4.best_estimator_

grid_model4.best_score_

grid_model4.best_params_

model4_grid = GradientBoostingClassifier(learning_rate=0.03, max_depth=8, n_estimators= 1000, subsample= 0.9)
model4_grid.fit(X_train,y_train)
prediction4_grid = model4_grid.predict(X_test)
print(classification_report(y_test, prediction4_grid))

acc_gb_grid = accuracy_score(y_test, prediction4_grid)
recall_gb_grid = recall_score(y_test, prediction4_grid)
precision_gb_grid = precision_score(y_test, prediction4_grid)
f1_score_gb_grid = f1_score(y_test, prediction4_grid)  
print(acc_gb_grid)

roc_gb_grid = roc_auc_score(y_test, prediction4_grid)
print(roc_gb_grid )

"""KNeighbors Classifier"""

model5 = KNeighborsClassifier(n_neighbors=10)
model5.fit(X_train,y_train)
prediction5 = model5.predict(X_test)
print(classification_report(y_test, prediction5))

acc_kneighbors = accuracy_score(y_test, prediction5)
recall_kneighbors = recall_score(y_test, prediction5)
precision_kneighbors = precision_score(y_test, prediction5)
f1_score_kneighbors = f1_score(y_test, prediction5)  
print(acc_kneighbors)

print(confusion_matrix(y_test, prediction5))

roc_knn = (roc_auc_score(y_test, prediction5))
print(roc_knn)

"""RandomForestClassifier

"""

model6 = RandomForestClassifier(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
model6.fit(X_train,y_train)
prediction6 = model6.predict(X_test)
print(classification_report(y_test, prediction6))

acc_rf = accuracy_score(y_test, prediction6)
recall_rf = recall_score(y_test, prediction6)
precision_rf = precision_score(y_test, prediction6)
f1_score_rf = f1_score(y_test, prediction6)

print(confusion_matrix(y_test, prediction6))

prediction6 = model6.predict(X_test)
print(classification_report(y_test, prediction6))

roc_rf = roc_auc_score(y_test, prediction6)
print(roc_rf)

rfc=RandomForestClassifier(random_state=45)
param_grid = { 'n_estimators': [100, 100],
               'max_features': ['auto', 'sqrt', 'log2'],
               'max_depth' : [4,5,6,7,8],
                'criterion' : ['gini', 'entropy']
              }

rfc_grid = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5)
rfc_grid.fit(X_train, y_train)

rfc_grid.best_params_

rfc = RandomForestClassifier(random_state=55, max_features='auto', n_estimators=100, max_depth=8, criterion='gini')

rfc.fit(X_train, y_train)

pred_rfc_grid=rfc.predict(X_test)
print(classification_report(y_test, pred_rfc_grid))

models = ['Logistic Regression', 'SVM', 'Decision Tree', 'GradientBoosting', 'KNeighbors', 'Random Forest']
accuracy = [acc_lr, acc_svm, acc_dt, acc_gb_grid, acc_kneighbors, acc_rf]
recall = [recall_lr, recall_svm, recall_dt, recall_gb_grid, recall_kneighbors, recall_rf]
precision = [precision_lr, precision_svm, precision_dt, precision_gb_grid, precision_kneighbors, precision_rf]
f1_score = [f1_score_lr, f1_score_svm, f1_score_dt, f1_score_gb_grid, f1_score_kneighbors, f1_score_rf]
roc_score = [roc_lr, roc_svm, roc_dt, roc_gb_grid, roc_knn, roc_rf]

accuarcy_compare = {'accuracy': accuracy, 'recall': recall, 'precision': precision, 'f1_score': f1_score}
df_metrics = pd.DataFrame(accuarcy_compare, index=models)
df_metrics

"""Summary : GradientBoosting have the highest accuracy score (88%). """